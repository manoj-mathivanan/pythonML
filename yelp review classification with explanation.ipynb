{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPqYyCMUC2Pp28AgAFNJOXn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manoj-mathivanan/pythonML/blob/main/yelp%20review%20classification%20with%20explanation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "!pip install datasets\n",
        "!pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uHNjqMsny67c",
        "outputId": "c977c1e3-87ee-4a48-d1fd-f4c49adc3e93"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.27.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.12.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.2.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython import get_ipython\n",
        "from IPython.display import display\n",
        "# %%\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "from datasets import load_dataset\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"
      ],
      "metadata": {
        "id": "rSxHjwqxyRn-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########################################################################\n",
        "# 1. Text Preprocessing\n",
        "########################################################################\n",
        "\n",
        "class TextPreprocessor:\n",
        "    \"\"\"\n",
        "    Preprocesses text data for sentiment analysis.\n",
        "\n",
        "    - Cleans and normalizes text.\n",
        "    - Builds vocabulary from a list of texts.\n",
        "    - Converts text into sequences of word indices.\n",
        "    \"\"\"\n",
        "    def __init__(self, max_vocab_size=10000, max_seq_length=128):\n",
        "        self.max_vocab_size = max_vocab_size\n",
        "        self.max_seq_length = max_seq_length\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = {}\n",
        "        self.vocab_size = 0\n",
        "\n",
        "    def clean_text(self, text):\n",
        "        \"\"\"Clean and normalize text\"\"\"\n",
        "        text = str(text).lower()\n",
        "        text = re.sub(r'<[^>]+>', ' ', text)  # Replace HTML tags with space\n",
        "        text = re.sub(r'http\\S+|www\\S+', ' ', text)  # Replace URLs with space\n",
        "        text = re.sub(r'[^\\w\\s]', ' ', text)  # Replace special chars with space\n",
        "        text = ' '.join(text.split())  # Remove extra whitespace\n",
        "\n",
        "        # Return '<UNK>' if text is empty after cleaning\n",
        "        return text if text.strip() else '<UNK>'\n",
        "\n",
        "    def build_vocabulary(self, texts):\n",
        "        \"\"\"Build vocabulary from list of texts\"\"\"\n",
        "        print(\"Building vocabulary...\")\n",
        "        word_counts = Counter()\n",
        "        for text in tqdm(texts):\n",
        "            words = self.clean_text(text).split()\n",
        "            word_counts.update(words)\n",
        "\n",
        "        # Add special tokens\n",
        "        self.word2idx['<PAD>'] = 0\n",
        "        self.word2idx['<UNK>'] = 1\n",
        "\n",
        "        # Add most common words\n",
        "        for word, _ in word_counts.most_common(self.max_vocab_size - 2):\n",
        "            self.word2idx[word] = len(self.word2idx)\n",
        "\n",
        "        self.idx2word = {idx: word for word, idx in self.word2idx.items()}\n",
        "        self.vocab_size = len(self.word2idx)\n",
        "        print(f\"Vocabulary size: {self.vocab_size}\")\n",
        "\n",
        "    def text_to_sequence(self, text):\n",
        "        \"\"\"Convert text to sequence of word indices\"\"\"\n",
        "        words = self.clean_text(text).split()\n",
        "\n",
        "        # Handle empty sequences by adding an unknown token\n",
        "        if len(words) == 0:\n",
        "            words = ['<UNK>']\n",
        "\n",
        "        sequence = [self.word2idx.get(word, self.word2idx['<UNK>'])\n",
        "                    for word in words]\n",
        "\n",
        "        # Get actual sequence length before padding\n",
        "        length = min(len(sequence), self.max_seq_length)\n",
        "\n",
        "        # Pad or truncate sequence\n",
        "        if len(sequence) < self.max_seq_length:\n",
        "            sequence = sequence + [self.word2idx['<PAD>']] * (self.max_seq_length - len(sequence))\n",
        "        else:\n",
        "            sequence = sequence[:self.max_seq_length]\n",
        "\n",
        "        return sequence, length\n",
        "\n"
      ],
      "metadata": {
        "id": "gJadhD6LySvR"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########################################################################\n",
        "# 2. Dataset Creation\n",
        "########################################################################\n",
        "\n",
        "class ReviewDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset class for Yelp reviews.\n",
        "    \"\"\"\n",
        "    def __init__(self, texts, ratings, preprocessor):\n",
        "        self.texts = texts\n",
        "        self.ratings = ratings  # 1-5 stars\n",
        "        self.preprocessor = preprocessor\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        rating = self.ratings[idx]\n",
        "\n",
        "        sequence, length = self.preprocessor.text_to_sequence(text)\n",
        "        sequence_tensor = torch.tensor(sequence, dtype=torch.long)\n",
        "        rating_tensor = torch.tensor(rating - 1, dtype=torch.long)  # Convert to 0-4\n",
        "        length_tensor = torch.tensor(length, dtype=torch.long)\n",
        "\n",
        "        return sequence_tensor, rating_tensor, length_tensor\n"
      ],
      "metadata": {
        "id": "VlIF3tUlyj9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########################################################################\n",
        "# 3. Model Definition\n",
        "########################################################################\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import gensim.downloader as api\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "import numpy as np\n",
        "\n",
        "class MulticlassSentimentRNN(nn.Module):\n",
        "    \"\"\"\n",
        "    RNN model for multiclass sentiment analysis.\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, vocab_to_idx, hidden_dim, n_classes, n_layers, dropout, device):\n",
        "        super().__init__()\n",
        "\n",
        "        # Load pre-trained Word2Vec embeddings from gensim\n",
        "        print(\"Loading pre-trained word vectors...\")\n",
        "        word2vec = api.load('word2vec-google-news-300')\n",
        "        embedding_dim = word2vec.vector_size\n",
        "\n",
        "        # Initialize embedding matrix with pre-trained embeddings\n",
        "        print(\"Initializing embedding matrix...\")\n",
        "        weight_matrix = torch.zeros((vocab_size, embedding_dim))\n",
        "        words_found = 0\n",
        "\n",
        "        for word, idx in vocab_to_idx.items():\n",
        "            try:\n",
        "                weight_matrix[idx] = torch.FloatTensor(word2vec[word])\n",
        "                words_found += 1\n",
        "            except KeyError:\n",
        "                weight_matrix[idx] = torch.randn(embedding_dim) * 0.1  # Random initialization for unknown words\n",
        "\n",
        "        print(f\"Found pre-trained vectors for {words_found}/{vocab_size} words\")\n",
        "\n",
        "        # Create embedding layer with pre-trained weights\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        self.embedding.weight = nn.Parameter(weight_matrix)\n",
        "        self.embedding.weight.requires_grad = False  # Freeze embeddings\n",
        "\n",
        "        # RNN layers\n",
        "        self.rnn = nn.RNN(embedding_dim, hidden_dim, num_layers=n_layers,\n",
        "                         bidirectional=True, dropout=dropout if n_layers > 1 else 0,\n",
        "                         batch_first=True)\n",
        "\n",
        "        self.fc = nn.Linear(hidden_dim * 2, hidden_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.classifier = nn.Linear(hidden_dim, n_classes)\n",
        "\n",
        "    def forward(self, text, lengths):\n",
        "        embedded = self.dropout(self.embedding(text))\n",
        "        packed_embedded = pack_padded_sequence(embedded, lengths.cpu(),\n",
        "                                           batch_first=True, enforce_sorted=False)\n",
        "        packed_output, hidden = self.rnn(packed_embedded)\n",
        "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
        "        hidden = self.dropout(hidden)\n",
        "        hidden = torch.relu(self.fc(hidden))\n",
        "        return self.classifier(hidden)\n"
      ],
      "metadata": {
        "id": "4pJAGV1Cyoj-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########################################################################\n",
        "# 4. Model Training\n",
        "########################################################################\n",
        "\n",
        "def train_model(model, train_loader, valid_loader, optimizer, device, n_epochs):\n",
        "    \"\"\"Train the model\"\"\"\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    best_valid_loss = float('inf')\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        correct_predictions = 0\n",
        "        total_predictions = 0\n",
        "\n",
        "        progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{n_epochs}')\n",
        "        for batch_idx, (texts, ratings, lengths) in enumerate(progress_bar):\n",
        "            texts = texts.to(device)\n",
        "            ratings = ratings.to(device)\n",
        "            lengths = lengths.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            predictions = model(texts, lengths)\n",
        "            loss = criterion(predictions, ratings)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            _, predicted_ratings = torch.max(predictions, 1)\n",
        "            correct_predictions += (predicted_ratings == ratings).sum().item()\n",
        "            total_predictions += ratings.size(0)\n",
        "\n",
        "            # Update progress bar\n",
        "            avg_loss = total_loss / (batch_idx + 1)\n",
        "            accuracy = correct_predictions / total_predictions\n",
        "            progress_bar.set_postfix({'loss': f'{avg_loss:.4f}',\n",
        "                                    'accuracy': f'{accuracy:.4f}'})\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        valid_loss = 0\n",
        "        valid_correct = 0\n",
        "        valid_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for texts, ratings, lengths in valid_loader:\n",
        "                texts = texts.to(device)\n",
        "                ratings = ratings.to(device)\n",
        "                lengths = lengths.to(device)\n",
        "\n",
        "                predictions = model(texts, lengths)\n",
        "                loss = criterion(predictions, ratings)\n",
        "\n",
        "                valid_loss += loss.item()\n",
        "                _, predicted_ratings = torch.max(predictions, 1)\n",
        "                valid_correct += (predicted_ratings == ratings).sum().item()\n",
        "                valid_total += ratings.size(0)\n",
        "\n",
        "        avg_valid_loss = valid_loss / len(valid_loader)\n",
        "        valid_accuracy = valid_correct / valid_total\n",
        "\n",
        "        print(f'\\nValidation Loss: {avg_valid_loss:.4f}')\n",
        "        print(f'Validation Accuracy: {valid_accuracy:.4f}')\n",
        "\n",
        "        # Save the best model\n",
        "        if avg_valid_loss < best_valid_loss:\n",
        "            best_valid_loss = avg_valid_loss\n",
        "            torch.save(model.state_dict(), 'best_model.pt')\n",
        "            print('Best model saved!')\n",
        "\n"
      ],
      "metadata": {
        "id": "_QTSoh4syucH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########################################################################\n",
        "# 5. Prediction\n",
        "########################################################################\n",
        "\n",
        "def predict_rating(model, preprocessor, text, device):\n",
        "    \"\"\"Predict rating for a single text\"\"\"\n",
        "    model.eval()\n",
        "    sequence, length = preprocessor.text_to_sequence(text)\n",
        "    sequence_tensor = torch.tensor([sequence], dtype=torch.long).to(device)\n",
        "    length_tensor = torch.tensor([length], dtype=torch.long).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        prediction = model(sequence_tensor, length_tensor)\n",
        "        probabilities = torch.softmax(prediction, dim=1)\n",
        "        predicted_rating = torch.argmax(prediction, dim=1).item() + 1\n",
        "        confidence = probabilities[0][predicted_rating-1].item()\n",
        "\n",
        "    return predicted_rating, confidence\n"
      ],
      "metadata": {
        "id": "d-F_dG_syzI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uB_KPLrfuJ4W"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "########################################################################\n",
        "# 6. Main Execution\n",
        "########################################################################\n",
        "\n",
        "def main():\n",
        "    # Set random seeds for reproducibility\n",
        "    torch.manual_seed(42)\n",
        "    np.random.seed(42)\n",
        "\n",
        "    # Load Yelp reviews dataset\n",
        "    print(\"Loading dataset...\")\n",
        "    dataset = load_dataset(\"yelp_review_full\")\n",
        "\n",
        "    # Use a subset of the training data\n",
        "    train_dataset = dataset[\"train\"].shuffle(seed=42).select(range(50000))\n",
        "\n",
        "    # Convert to lists\n",
        "    texts = train_dataset['text']\n",
        "    ratings = [r + 1 for r in train_dataset['label']]  # Convert from 0-4 to 1-5\n",
        "\n",
        "    # Print dataset statistics\n",
        "    print(f\"\\nTotal samples: {len(texts)}\")\n",
        "    print(\"\\nRating distribution:\")\n",
        "    for i in range(1, 6):\n",
        "        count = sum(1 for r in ratings if r == i)\n",
        "        print(f\"{i} stars: {count} reviews ({count/len(ratings)*100:.1f}%)\")\n",
        "\n",
        "    # Split data\n",
        "    train_texts, valid_texts, train_ratings, valid_ratings = train_test_split(\n",
        "        texts, ratings, test_size=0.2, random_state=42, stratify=ratings\n",
        "    )\n",
        "\n",
        "    # Initialize preprocessor\n",
        "    preprocessor = TextPreprocessor(max_vocab_size=25000, max_seq_length=128)\n",
        "    preprocessor.build_vocabulary(train_texts)\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = ReviewDataset(train_texts, train_ratings, preprocessor)\n",
        "    valid_dataset = ReviewDataset(valid_texts, valid_ratings, preprocessor)\n",
        "\n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "    valid_loader = DataLoader(valid_dataset, batch_size=32)\n",
        "\n",
        "    # Initialize model\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"\\nUsing device: {device}\")\n",
        "\n",
        "    model = MulticlassSentimentRNN(\n",
        "      vocab_size=preprocessor.vocab_size,\n",
        "      vocab_to_idx=preprocessor.word2idx,\n",
        "      hidden_dim=256,\n",
        "      n_classes=5,\n",
        "      n_layers=3,\n",
        "      dropout=0.5,\n",
        "      device=device).to(device)\n",
        "\n",
        "    # Initialize optimizer\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    # Train model\n",
        "    train_model(\n",
        "        model=model,\n",
        "        train_loader=train_loader,\n",
        "        valid_loader=valid_loader,\n",
        "        optimizer=optimizer,\n",
        "        device=device,\n",
        "        n_epochs=5\n",
        "    )\n",
        "\n",
        "    # Test the model with some example reviews\n",
        "    test_reviews = [\n",
        "        \"The food was absolutely amazing! The service was impeccable and the atmosphere was perfect. Will definitely come back!\",\n",
        "        \"Terrible experience. Rude staff, cold food, and overpriced. Would not recommend to anyone.\",\n",
        "        \"It's an okay place. The food is decent but nothing special. Prices are reasonable.\",\n",
        "        \"Good restaurant with friendly staff. The food could be better but overall a pleasant experience.\",\n",
        "        \"Average place. Service was slow but the food was decent. Might give it another try.\"\n",
        "    ]\n",
        "\n",
        "    print(\"\\nTesting model with example reviews:\")\n",
        "    for review in test_reviews:\n",
        "        rating, confidence = predict_rating(model, preprocessor, review, device)\n",
        "        print(f\"\\nReview: {review}\")\n",
        "        print(f\"Predicted Rating: {rating} stars (confidence: {confidence:.2f})\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ]
}